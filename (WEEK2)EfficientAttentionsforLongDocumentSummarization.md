NAACL2021
https://arxiv.org/pdf/2104.02112.pdf

**Problem**<br>
The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization

Solution:

**Contribution/Result**
- we are able to process ten times more tokens than existing models that use full attentions
- we present a new dataset, GOVREPORT, with significantly longer documents and summaries
- our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed
