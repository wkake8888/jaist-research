ACL2021
https://aclanthology.org/2021.acl-long.227.pdf

**Problem**
- Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption.
- Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes.

