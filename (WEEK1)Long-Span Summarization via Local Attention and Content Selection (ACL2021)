**main content:**
 two methods for long-span summarization tasks. First, on local self-attention transformers, we present the design considerations for local
self-attention BART, and we investigate the feasibility and performance of different network configurations. Second, on content selection, we distinguish
between training time and test time methods, and we provide a good practice for both phases.


