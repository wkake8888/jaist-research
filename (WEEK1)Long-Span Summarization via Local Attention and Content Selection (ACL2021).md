LONG-SPAN
----
https://arxiv.org/abs/2105.03801
**main content:**
 two methods for long-span summarization tasks. 
 
 First, on local self-attention transformers,  present the design considerations for local
self-attention BART, and we investigate the feasibility and performance of different network configurations. 

 Second, on content selection, we distinguish between training time and test time methods,
 and we provide a good practice for both phases.

detail
------
